# include the infra  playbook so we have some registered variables
# - include: infra.yml

- hosts: "{{ target_cluster }}"
  gather_facts: no
  tasks:

  # temporarily increase the desired_capacity and minimum_size by
  # the batch_size
  - name: set temporary settings
    ec2_asg: name={{ asg_group_name }} 
             region={{ region }} 
             desired_capacity={{ tmp_desired_capacity }}
             min_size={{ tmp_min_size }}

  # we need to wait for the ASG health_check_grace period + LB Unhealthythreshold * LB HealthCheck Interval
  # + Buffer or else instances will always report healthy
  - pause: seconds=30
  - name: wait for number_of_healthy_instances >= desired_capacity
    ec2_asg_facts: name={{ asg_group_name }} region={{ region }}
    register: result
    until: result.healthy_instances >= tmp_min_size|int and result.in_service_instances >= tmp_min_size|int
    delay: 10
    retries: 120

# add hosts to a group created by group_by for their ec2_image_id
# and use intersection of imaged_id and autoscale_groupName as 
# host target (ensures we are tackling old hosts)
- hosts: "{{ asg_hosts }}"
  gather_facts: no
  tasks:
  - group_by: key={{ hostvars[inventory_hostname]['ec2_image_id'] }}

#instead of this, we could have a module that returned all autoscale instances in the group
# that did not have an AMI that is specified.
# then we would loop through that list of AMIs and would not have to rely on the inventory


- include: terminator.yml min_num_instances="{{ tmp_min_size }}"


# set the ASG to normal settings
- hosts: "{{ target_cluster }}"
  gather_facts: no
  tasks:
  - name: return settings to normal
    ec2_asg: name={{ asg_group_name }} region={{ region }} 
             desired_capacity={{ asg_desired_capacity }}
             min_size={{ asg_min_size }}


